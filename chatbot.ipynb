{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R_yADIUrM0KX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in ./opt/anaconda3/lib/python3.9/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: jinja2 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.30.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: setuptools in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_md \n",
    "\n",
    "#Feet Forward Neural Network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size , num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) #layers of the NN\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()#Activation Function \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        #  no activation and no softmax\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW-xLoA-PRVi",
    "outputId": "4d4c2b40-a069-4737-df3e-e84cadb65887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\"\"\"Natural language tool kit is the pyhton libary used for the natural language processing\"\"\"\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# importing the library  \n",
    "import language_tool_python   \n",
    "# creating the tool  \n",
    "my_tool = language_tool_python.LanguageTool('en-US') \n",
    "import numpy as np\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(sentence):\n",
    "    return regexp_tokenize(sentence, pattern=\"\\w+\")\n",
    "def stem(word):\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'\\d+', '', word)\n",
    "    return stemmer.stem(word)\n",
    "def bag_of_words(tokenized_sentence,all_words):\n",
    "    tokenized_sentence = [my_tool.correct(word) for word in tokenized_sentence]\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(all_words),dtype=np.float32)\n",
    "\n",
    "    for idx,w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] =1.0\n",
    "    return bag\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dJuHeJqPRYK",
    "outputId": "46752b9f-7f5e-4b8a-e2e5-411bf7e49124"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0003\n",
      "Epoch [200/1000], Loss: 0.0002\n",
      "Epoch [300/1000], Loss: 0.0001\n",
      "Epoch [400/1000], Loss: 0.0001\n",
      "Epoch [500/1000], Loss: 0.0000\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0000\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.0000\n",
      "final loss: 0.0000\n",
      "training complete. file saved to /Users/mac/Desktop/SCIENTIFIC RESEARCH/data.pth\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "with open('/Users/mac/Desktop/SCIENTIFIC RESEARCH/QA.json','r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words =  []\n",
    "\n",
    "tags = []\n",
    "\n",
    "xy = [] #will hold both the pattern and then tags \n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))# here we are appending the word with its tag\n",
    "#ignore_words = ['?','!','.',',']\n",
    "ignore_words = stopwords.words('english')\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "\n",
    "\"\"\"\n",
    "for w in all_words:\n",
    "    if w not in ignore_words:\n",
    "        all_words.append(stem(w))\n",
    "\"\"\" \n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "# print(tags)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(x_train)\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "    #dataset[idx]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "#Hyperparameters\n",
    "batch_size = 64\n",
    "hidden_size = 1024\n",
    "output_size = len(tags)\n",
    "input_size = len(X_train[0])\n",
    "# print(input_size , len(all_words))\n",
    "# print(output_size, tags)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset , batch_size=batch_size, shuffle=True, num_workers = 0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"/Users/mac/Desktop/SCIENTIFIC RESEARCH/data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am Fernando\n",
      "I am here to help you feel free to chat with me:-\n",
      "Let's chat! (type 'quit' to exit)\n",
      "\n",
      "requirement\n",
      "\n",
      "scholarships\n",
      "\n",
      "batchmates\n",
      "\n",
      "language\n",
      "\n",
      "research topics\n",
      "\n",
      "internship\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "#----------------\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('/Users/mac/Desktop/SCIENTIFIC RESEARCH/QA.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"/Users/mac/Desktop/SCIENTIFIC RESEARCH/data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Fernando\"\n",
    "print(\"Hi! I am Fernando\")\n",
    "print(\"I am here to help you feel free to chat with me:-\")\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "print()\n",
    "\n",
    "with open('/Users/mac/Desktop/SCIENTIFIC RESEARCH/main QA.json', 'r') as json_data:\n",
    "    main_intents = json.load(json_data)\n",
    "corpse = []\n",
    "responses = []\n",
    "for intent in main_intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    response = intent['responses']\n",
    "    print(tag+\"\\n\")\n",
    "    corpse.append(tag)# here we are appending the word with its tag\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "Fernando: Please write your question with keyword above\n",
      "You: internship\n",
      "Fernando: Sure, the most convenient option would be to intern at one of our scientific labs where you will also have a chance to work on your Master's thesis.\n",
      "You: batchmates\n",
      "Fernando: You will be studying with Russian-speaking students, as well as international students from all over the world.\n",
      "You: thanks\n",
      "Fernando: You're most welcome!\n",
      "You: is there club\n",
      "Fernando: Please take a looks at this website\n",
      "https://student.itmo.ru/en/clubs/\n",
      "You: how can i improve my skill\n",
      "Fernando: you will solve applied problems of processing and visualizing big data; master the methods of formalization, structuring, interpretation, and assimilation of knowledge for various tasks; will be able to develop and implement effective algorithms for multivariate analysis of complex data, and also solve actual tasks related to the principles of construction and the basics of organizing the development of modern software solutions for processing big data.\n",
      "You: what is deadline of document submission\n",
      "Fernando: August 11, 2023\n",
      "You: when is date of exam\n",
      "Fernando: The exams are being held from March 1st to August 25th 2023. If you want to know the date of the nearest exam, please contact the program manager via email: aakarabintseva@itmo.ru (Ms. Alexandra Karabintseva)\n",
      "You: where can i submit my document\n",
      "Fernando: You need to fill out and submit your application at https://signup.itmo.ru/master\n",
      "For further information please contact the International Office via email: international@itmo.ru.\n",
      "You: I am afraid that I can not adapt life there\n",
      "Fernando: ITMO University holds the “Buddy System” to help newly arrived students settle down in a new environment for them. Please visit the web-page to learn details: https://news.itmo.ru/en/blog/345/. In addition you can explore the international part of the site of the service “ITMO.Student”: https://student.itmo.ru/en/ .\n",
      "You: list of classes\n",
      "Fernando: The program content is available in the web-page: https://en.itmo.ru/en/viewjep/2/5/Big_Data_and_Machine_Learning.htm.\n",
      "You: is there another way to enroll\n",
      "Fernando: ITMO University offers two alternative forms of entrance trials:1) Portfolio contest for international applicants. Learn more: https://int.itmo.ru/en/opportunities_for_applicants_masters_portfolio;2) OpenDoors Olympiad for international applicants. Learn more: https://int.itmo.ru/en/opendoors;Please contact the program manager for details via email: aakarabintseva@itmo.ru (Ms. Alexandra Karabintseva).\n",
      "You: is there scholarship\n",
      "Fernando: We don't have our own scholarship funding within the program, but you can apply through the Russian government international scholarship website and choose our program in your application (http://studyinrussia.ru/en/actual/scholarships/).\n",
      "You: What scholarships are proposed by ITMO University? Where can I find the conditions to apply for them?\n",
      "Fernando: ITMO University does not offer any scholarships. But you can find out information about scholarships of the Russian Government. Please visit the webpage: https://studyinrussia.ru/en/actual/scholarships/.\n",
      "You: bye\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "token = lambda word: nlp(word)[0]  # shortcut to convert string to spacy.Token\n",
    "score_words = lambda w1, w2: token(w1).similarity(token(w2))\n",
    "while True:\n",
    "    \n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    #sentence = input(\"You: \")\n",
    "    # given text  \n",
    "    #----\n",
    "    sentence = input(\"You: \")\n",
    "#student = tokenize(student)\n",
    "#student = [stem(w) for w in student if w not in ignore_words]\n",
    "#student = sorted(set(student))\n",
    "    greeting = [\"Hi\", \"Hey\", \"Is anyone there?\", \"Hello\", \"Hay\",\"Hi there\"]\n",
    "    if(any(sentence.lower()==item.lower() for item in [\"quit\",\"finish\",\"over\",\"bye\",\"goodbye\"])):\n",
    "        print(f\"{bot_name}: Goodbye , have a nice day\")\n",
    "            break\n",
    "    \n",
    "    similarity = []\n",
    "    for i in corpse:\n",
    "        similarity.append(score_words(sentence,i))\n",
    "    #print(similarity)\n",
    "    if(any(sentence.lower()==item.lower() for item in greeting)):\n",
    "        print(f\"{bot_name}: Please write your question with keyword above\")\n",
    "    elif(max(similarity) > 0.5 and len(tokenize(sentence))==1 ):\n",
    "        print(f\"{bot_name}: \"+responses[similarity.index(max(similarity))][0])\n",
    "    else:\n",
    "        \n",
    "        sentence = my_tool.correct(sentence)  \n",
    "        #------\n",
    "\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.3:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Sorry I am unable to Process Your Request\")\n",
    "            print(f\"{bot_name}: You may find the way forward in https://en.itmo.ru/en/viewjep/2/5/Big_Data_and_Machine_Learning.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
